require "stash.jinc"
require "position_map.jinc"

inline
fn oram_clear(
  reg u64 oram
)
{
  reg u64 bucket_store stash allocated_ub;
  bucket_store = (64u)[oram];
  bucket_store_clear(bucket_store);
  stash = (64u)[oram + 8 * STASH_ADDR];
  stash_clear(stash);
  (u64)[oram + 8 * ALLOCATED_UB_ADDR] = 0;
}

inline
fn random_mod_by_pow_of_2(
  reg u64 modulus
) -> reg u64
{
  stack u8[8] random;
  reg ptr u8[8] randomp;
  reg u64 r;

  randomp = random;
  randomp = #randombytes(randomp);
  r = randomp[u64 0];
  modulus -= 1;
  r &= modulus;
  return r;
}

/**
 * @brief read the path from the bucket store, performing the same sequence of instructions independent of the input.
 * Post-condition: the block with `id == target_block_id` will *not* be in the stash - neither the overflow or the path stash.
 * It will be in the block `*target` and the new position will be set.
 * 
 * @param oram 
 * @param path Path for block with ID `target_block_id`.
 * @param target_block_id ID of block to read
 * @param target On output, block with ID `target_block_id` will be available here
 * @param new_position Position for the target block after this access
 */
fn oram_read_path_for_block(
  reg u64 oram path,
  reg u64 target_block_id,
  reg u64 target,
  reg u64 new_position
)
{
  reg u64 stash bucket_store value;
  inline int i;

  () = #spill(new_position);
  stash = (64u)[oram + 8 * STASH_ADDR];
  bucket_store = [oram];
  for i = 0 to PATH_LENGTH
  {
    value = (64u)[path + 8 + 8 * i];
    stash_add_path_bucket(stash, bucket_store, value, target_block_id, target);
  }
  stash_scan_overflow_for_target(stash, target_block_id, target);

  () = #unspill(new_position);
  (u64)[target] = target_block_id;
  (u64)[target + 8] = new_position;
}

// fn oram_access_read(
//   reg u64 oram,
//   reg u64 block_id,
//   reg u64 block_data,
//   reg u64 out_data
// )
// {
//   stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
//   reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
//   // standard variables
//   reg u64 max_position new_position x;
//   // pointer variables
//   reg u64 stash path position_map bucket_store path_blocks;
//   // temporary variables
//   reg u64 bucket_id;
//   inline int i offset;

//   target_block = target_block_s;
//   for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
//   max_position = (64u)(1 << (PATH_LENGTH - 1));

//   new_position = random_mod_by_pow_of_2(oram, max_position);
//   x = #set0();
//   position_map = [oram + 8 * POSITION_MAP_ADDR];
//   // must generallize to incorporate oram position map
//   scan_position_map_set(position_map, block_id, new_position, &x); // fix x address
//   x *= 2;

//   path = [oram + 8 * PATH_ADDR];
//   tree_path_update(path, x);

//   new_position <<= 1;
//   oram_read_path_for_block(oram, ptr, block_id, target_block, new_position);
//   read_accessor(target_block);

//   stash = [oram + 8 * STASH_ADDR];
//   stash_add_block(stash, target_block);

//   stash_build_path(stash, path);

//   bucket_store = [oram];
//   path_blocks = [stash + 8 * PATH_BLOCKS_ADDR];
//   offset = BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
//   for i = 0 to PATH_LENGTH
//   {
//     bucket_id = [path + 8 + 8 * i];
//     bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks)
//     path_blocks += (64u)offset;
//   }
// }

inline
fn read_accessor(
  reg u64 block_data,
  reg u64 out_data
)
{
  reg u64 r64;
  inline int i;
  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = (64u)[block_data + 8 * i];
    (u64)[out_data + 8 * i] = r64;
  }
}

inline
fn write_accessor_full(
  reg u64 block_data,
  reg u64 vargs
)
{
  reg u64 in_data r64;
  inline int i;

  in_data = [vargs + 8 * 2];
  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = (64u)[in_data + 8 * i];
    (u64)[block_data + 8 * i] = r64;
  }
}

// inline
// fn write_accessor(
//   reg u64 block_data,
//   reg u64 vargs
// )
// {
//   reg u64 in_data_start in_data_len in_data out_data;
//   reg u64 tmp addr cons;
//   reg u8 c1 c2;
//   reg bool b;
//   inline int i;

//   block_data = block_data;
//   vargs = vargs;
//   out_data = [vargs + 8 * 3];
//   if (out_data != 0) {
//     read_accessor(block_data, out_data);
//   }

//   in_data_start = [vargs];
//   in_data_len = [vargs + 8];
//   in_data = [vargs + 8 * 2];

//   cons = in_data_start % in_data_len;
//   cons = #LEA(in_data_len - cons);
//   for i = 0 to BLOCK_DATA_SIZE_QWORDS
//   {
//     // source_index
//     tmp = #LEA(cons + i);
//     tmp = tmp; in_data_len = in_data_len;
//     tmp = tmp % in_data_len;
//     tmp = tmp; in_data_len = in_data_len;

//     // address
//     tmp *= 8;
//     addr = #LEA(tmp + in_data);

//     // cond
//     b = i >= in_data_start;
//     c1 = #SETcc(b);
//     tmp = #LEA(in_data_start + in_data_len);
//     b = i < tmp;
//     c2 = #SETcc(b);
//     c1 &= c2;

//     cond_obv_cpy_u64(c1, block_data, addr);
//     block_data = #LEA(block_data + 8);
//   }
// }

inline
fn oram_allocate_block(
  reg u64 oram
) -> reg u64
{
  reg u64 allocated_ub requested_ub;
  allocated_ub = (64u)[oram + 8 * ALLOCATED_UB_ADDR];
  if (allocated_ub < CAPACITY_BLOCKS) {
    requested_ub = #LEA(allocated_ub + 1);
    (u64)[oram + 8 * ALLOCATED_UB_ADDR] = requested_ub;
  } else {
    allocated_ub = -1;
  }
  return allocated_ub;
}

inline
fn oram_allocate_contiguous(
  reg u64 oram,
  reg u64 num_blocks
) -> reg u64
{
  reg u64 requested_ub next_block allocated_ub;

  allocated_ub = (64u)[oram + 8 * ALLOCATED_UB_ADDR];
  requested_ub = #LEA(allocated_ub + num_blocks);
  if (requested_ub <= CAPACITY_BLOCKS) {
    next_block = allocated_ub;
    (u64)[oram + 8 * ALLOCATED_UB_ADDR] = requested_ub;
  } else {
    next_block = -1;
  }
  return next_block;
}
