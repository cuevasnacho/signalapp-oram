require "stash.jinc"
require "position_map.jinc"

inline
fn oram_clear(
  reg u64 oram
)
{
  reg u64 bucket_store stash;
  bucket_store = (64u)[oram];
  bucket_store_clear(bucket_store);
  stash = (64u)[oram + 8 * STASH_ADDR];
  stash_clear(stash);
  (u64)[oram + 8 * ALLOCATED_UB_ADDR] = 0;
}

inline
fn _random_mod_by_pow_of_2(
  reg u64 modulus
) -> reg u64
{
  stack u8[8] random;
  reg ptr u8[8] randomp;
  reg u64 r;

  randomp = random;
  randomp = #randombytes(randomp);
  r = randomp[u64 0];
  modulus -= 1;
  r &= modulus;
  return r;
}

/**
 * @brief read the path from the bucket store, performing the same sequence of instructions independent of the input.
 * Post-condition: the block with `id == target_block_id` will *not* be in the stash - neither the overflow or the path stash.
 * It will be in the block `*target` and the new position will be set.
 * 
 * @param oram 
 * @param path Path for block with ID `target_block_id`.
 * @param target_block_id ID of block to read
 * @param target On output, block with ID `target_block_id` will be available here
 * @param new_position Position for the target block after this access
 */

inline
fn _oram_read_path_for_block(
  reg u64 oram path,
  reg u64 target_block_id,
  reg u64 target,
  reg u64 new_position
)
{
  reg u64 stash bucket_store value;
  inline int i;

  () = #spill(new_position);
  stash = (64u)[oram + 8 * STASH_ADDR];
  bucket_store = [oram];
  for i = 0 to PATH_LENGTH
  {
    value = (64u)[path + 8 + 8 * i];
    stash_add_path_bucket(stash, bucket_store, value, target_block_id, target);
  }
  stash_scan_overflow_for_target(stash, target_block_id, target);

  () = #unspill(new_position);
  (u64)[target] = target_block_id;
  (u64)[target + 8] = new_position;
}

inline
fn _i_oram_read_path_for_block(
  reg u64 oram path,
  reg u64 target_block_id,
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target,
  reg u64 new_position
) -> reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS]
{
  reg u64 stash bucket_store value;
  inline int i;

  () = #spill(new_position);
  stash = (64u)[oram + 8 * STASH_ADDR];
  bucket_store = [oram];
  for i = 0 to PATH_LENGTH
  {
    value = (64u)[path + 8 + 8 * i];
    target = _i_stash_add_path_bucket(stash, bucket_store, value, target_block_id, target);
  }
  target = _i_stash_scan_overflow_for_target(stash, target_block_id, target);

  () = #unspill(new_position);
  target[u64 0] = target_block_id;
  target[u64 1] = new_position;
  return target;
}

inline
fn _read_accessor(
  reg u64 block_data,
  reg u64 out_data
)
{
  reg u64 r64;
  inline int i;
  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = (64u)[block_data + 8 * i];
    (u64)[out_data + 8 * i] = r64;
  }
}

inline
fn _i_read_accessor(
  reg ptr u64[BLOCK_DATA_SIZE_QWORDS] block_data,
  reg u64 out_data
) -> reg ptr u64[BLOCK_DATA_SIZE_QWORDS]
{
  reg u64 r64;
  inline int i;
  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = block_data[u64 i];
    (u64)[out_data + 8 * i] = r64;
  }
  return block_data;
}

inline
fn _write_accessor_full(
  reg u64 block_data,
  reg u64 in_data
)
{
  reg u64 r64;
  inline int i;

  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = (64u)[in_data + 8 * i];
    (u64)[block_data + 8 * i] = r64;
  }
}

inline
fn _i_write_accessor_full(
  reg ptr u64[BLOCK_DATA_SIZE_QWORDS] block_data,
  reg u64 in_data
) -> reg ptr u64[BLOCK_DATA_SIZE_QWORDS]
{
  reg u64 r64;
  inline int i;

  for i = 0 to BLOCK_DATA_SIZE_QWORDS
  {
    r64 = (64u)[in_data + 8 * i];
    block_data[u64 i] = r64;
  }
  return block_data;
}

inline
fn _oram_access_read(
  reg u64 oram,
  reg u64 block_id,
  reg u64 out_data
)
{
  stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
  // standard variables
  reg u64 max_position new_position x;
  // pointer variables
  reg u64 stash path position_map bucket_store path_blocks;
  // temporary variables
  reg u64 bucket_id;
  inline int i offset;

  target_block = target_block_s;
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
  max_position = (64u)(1 << (PATH_LENGTH - 1));

  new_position = _random_mod_by_pow_of_2(max_position);
  position_map = [oram + 8 * POSITION_MAP_ADDR];
  () = #spill(oram, block_id, new_position, target_block, out_data);
  x = _i_position_map_read_then_set(position_map, block_id, new_position);
  x *= 2;

  () = #unspill(oram, block_id, new_position, target_block);
  path = [oram + 8 * PATH_ADDR];
  tree_path_update(path, x);

  new_position *= 2;
  target_block = _i_oram_read_path_for_block(oram, path, block_id, target_block, new_position);
  () = #spill(path);
  () = #unspill(out_data);
  target_block[2:BLOCK_DATA_SIZE_QWORDS] = _i_read_accessor(target_block[2:BLOCK_DATA_SIZE_QWORDS], out_data);

  stash = [oram + 8 * STASH_ADDR];
  () = #spill(oram);
  target_block = _i_stash_add_block(stash, target_block);

  () = #unspill(path);
  stash_build_path(stash, path);
  () = #unspill(oram);

  bucket_store = [oram];
  path_blocks = [stash + 8 * PATH_BLOCKS_ADDR];
  offset = BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
  for i = 0 to PATH_LENGTH
  {
    bucket_id = [path + 8 + 8 * i];
    bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks);
    path_blocks += (64u)offset;
  }
}

inline
fn _oram_access_write(
  reg u64 oram,
  reg u64 block_id,
  reg u64 in_data
)
{
  stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
  // standard variables
  reg u64 max_position new_position x;
  // pointer variables
  reg u64 stash path position_map bucket_store path_blocks;
  // temporary variables
  reg u64 bucket_id;
  inline int i offset;

  target_block = target_block_s;
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
  max_position = (64u)(1 << (PATH_LENGTH - 1));

  new_position = _random_mod_by_pow_of_2(max_position);
  position_map = [oram + 8 * POSITION_MAP_ADDR];
  () = #spill(oram, block_id, new_position, target_block, in_data);
  x = _i_position_map_read_then_set(position_map, block_id, new_position);
  x *= 2;

  () = #unspill(oram, block_id, new_position, target_block);
  path = [oram + 8 * PATH_ADDR];
  tree_path_update(path, x);

  new_position *= 2;
  target_block = _i_oram_read_path_for_block(oram, path, block_id, target_block, new_position);
  () = #spill(path);
  () = #unspill(in_data);
  target_block[2:BLOCK_DATA_SIZE_QWORDS] = _i_write_accessor_full(target_block[2:BLOCK_DATA_SIZE_QWORDS], in_data);

  stash = [oram + 8 * STASH_ADDR];
  () = #spill(oram);
  target_block = _i_stash_add_block(stash, target_block);

  () = #unspill(path);
  stash_build_path(stash, path);
  () = #unspill(oram);

  bucket_store = [oram];
  path_blocks = [stash + 8 * PATH_BLOCKS_ADDR];
  offset = BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
  for i = 0 to PATH_LENGTH
  {
    bucket_id = [path + 8 + 8 * i];
    bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks);
    path_blocks += (64u)offset;
  }
}

inline
fn oram_allocate_block(
  reg u64 oram
) -> reg u64
{
  reg u64 allocated_ub requested_ub;
  allocated_ub = (64u)[oram + 8 * ALLOCATED_UB_ADDR];
  if (allocated_ub < CAPACITY_BLOCKS) {
    requested_ub = #LEA(allocated_ub + 1);
    (u64)[oram + 8 * ALLOCATED_UB_ADDR] = requested_ub;
  } else {
    allocated_ub = -1;
  }
  return allocated_ub;
}

inline
fn oram_allocate_contiguous(
  reg u64 oram,
  reg u64 num_blocks
) -> reg u64
{
  reg u64 requested_ub next_block allocated_ub;

  allocated_ub = (64u)[oram + 8 * ALLOCATED_UB_ADDR];
  requested_ub = #LEA(allocated_ub + num_blocks);
  if (requested_ub <= CAPACITY_BLOCKS) {
    next_block = allocated_ub;
    (u64)[oram + 8 * ALLOCATED_UB_ADDR] = requested_ub;
  } else {
    next_block = -1;
  }
  return next_block;
}
