require "oram_position_map_1.jinc"

// tree path
inline
fn __tree_path_update(
  reg u64 t,
  reg u64 leaf
)
{
  reg u64 offset;
  inline int root root_level level;

  root_level = PATH_LENGTH_0 - 1;
  root = ((64u)1 << root_level) - 1;

  offset = coords_for_val(leaf);
  t = #LEA(t + 8);
  (u64)[t] = leaf;
  for level = 1 to root_level + 1
  {
    offset = offset >> 1;
    (u64)[t + 8 * level] = node_val(level, offset);
  }
}

// stash
inline
fn __stash_extend_overflow(
  reg u64 stash
)
{
  reg u64 old_num_blocks new_num_blocks;
  reg u64 addr old_len new_len flags new_addr prot fd offset err tmp;
  reg u64 blocks bucket_assignments;
  stack u64 blocks_s;
  inline int i;

  old_num_blocks = [stash + 8 * NUM_BLOCKS_ADDR];
  new_num_blocks = #LEA(old_num_blocks + STASH_GROWTH_INCREMENT);

  // (re)allocate new space, free the old
  blocks = [stash];
  old_len = old_num_blocks * DECRYPTED_BLOCK_SIZE;
  () = #spill(old_num_blocks);
  new_len = new_num_blocks * DECRYPTED_BLOCK_SIZE;
  flags = 1;
  new_addr = 0;
  blocks = blocks;
  new_addr = #mremap(blocks, old_len, new_len, flags, new_addr);
  [stash] = new_addr;
  blocks_s = new_addr;

  // free bucket_assignments
  bucket_assignments = [stash + 8 * BUCKET_ASSIGNMENTS_ADDR];
  () = #unspill(old_num_blocks);
  old_len = old_num_blocks * 8;
  err = #munmap(bucket_assignments, old_len);

  // mmap memory for bucket_assignments
  addr = 0;
  new_len = new_num_blocks * 8;
  prot = 0x1 | 0x2;   // PROT_READ | PROT_WRITE
  flags = 0x2 | 0x20; // MAP_PRIVATE | MAP_ANONYMOUS
  fd = -1;
  offset = 0;
  new_addr = #mmap(addr, new_len, prot, flags, fd, offset);
  [stash + 8 * BUCKET_ASSIGNMENTS_ADDR] = new_addr;

  // update our alias pointers
  blocks = blocks_s;
  tmp = #LEA(blocks + PATH_LENGTH_0 * BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE);
  [stash + 8 * PATH_BLOCKS_ADDR] = blocks;
  [stash + 8 * OVERFLOW_BLOCKS_ADDR] = tmp;

  // initialize new memory
  () = #unspill(old_num_blocks);
  old_num_blocks *= DECRYPTED_BLOCK_SIZE;
  blocks = #LEA(blocks + old_num_blocks);
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS * STASH_GROWTH_INCREMENT
  {
    [blocks + 8 * i] = -1;
  }

  // update counts
  [stash + 8 * NUM_BLOCKS_ADDR] = new_num_blocks;
  [stash + 8 * OVERFLOW_CAPACITY_ADDR] += STASH_GROWTH_INCREMENT;
}

inline
fn __stash_assign_block_to_bucket_overflow(
  #spill_to_mmx reg u64 stash path,
  reg u64 index,
  #msf reg u64 msf
) -> #msf reg u64
{
  // spill_to_mmx variables
  #spill_to_mmx reg u64 assignment_index bucket_assignments bid bpos;
  // pointer variables
  reg u64 path_blocks bucket_occupancy;
  // temporary variables
  reg u64 r1 r2 bucket_id tree_bound tmp;
  // boolean variables
  reg u8 c1 c2 c3;
  reg bool b;
  inline int lvl max_level;
  
  // the block cannot be assigned to this level or higher
  max_level = (64u)PATH_LENGTH_0;
  assignment_index = #LEA(BLOCKS_PER_BUCKET * PATH_LENGTH_0 + index);

  path_blocks, msf = stash_path_blocks(stash, msf);
  tmp = assignment_index;
  tmp *= 8 * DECRYPTED_BLOCK_SIZE_QWORDS;
  bid = (64u)[path_blocks + tmp];
  bid = #protect(bid, msf);
  () = #spill(bid);
  bpos = (64u)[path_blocks + tmp + 8];
  bpos = #protect(bpos, msf);
  () = #spill(bpos);

  bucket_occupancy, msf = stash_bucket_occupancy(stash, msf);
  bucket_assignments, msf = stash_bucket_assignments(stash, msf);
  () = #spill(path, assignment_index, bucket_assignments);

  c1 = #set0_8(); // is_assigned
  for lvl = 0 to max_level {
    r2 = (64u)[bucket_occupancy + 8 * lvl];
    r2 = #protect(r2, msf);
    () = #unspill(path);
    bucket_id = (64u)[path + 8 + 8 * lvl];
    bucket_id = #protect(bucket_id, msf);

    c1 = !c1;
    // is_valid
    tree_bound = tree_path_lower_bound(bucket_id);
    () = #unspill(bpos);
    b = tree_bound <= bpos;
    c2 = #SETcc(b);
    c2 &= c1;
    tree_bound = tree_path_upper_bound(bucket_id);
    () = #unspill(bpos);
    b = tree_bound >= bpos;
    c3 = #SETcc(b);
    c2 &= c3;
    // bucket_has_room
    b = r2 < BLOCKS_PER_BUCKET;
    c3 = #SETcc(b);
    c2 &= c3;
    // not is_empty
    () = #unspill(bid);
    b = bid != EMPTY_BLOCK_ID;
    c3 = #SETcc(b);
    c2 &= c3;
    // is_assigned = cond | is_assigned;
    c1 = !c1;
    c1 |= c2;
    b = c2 != 0;
    
    // If `b` is true, put it in the bucket: increment the bucket occupancy and set the bucket assignment
    // for this position.
    // increment this, it will only get saved if `b` is true.
    r1 = #LEA(r2 + 1);
    r1 = #CMOVcc(b, r1, r2);
    () = #unspill(assignment_index, bucket_assignments);
    (u64)[bucket_occupancy + 8 * lvl] = r1;
    r1 = (64u)[bucket_assignments + 8 * assignment_index];
    r1 = #protect(r1, msf);
    b = c2 != 0;
    tmp = lvl;
    r1 = #CMOVcc(b, tmp, r1);
    (u64)[bucket_assignments + 8 * assignment_index] = r1;
  }

  return msf;
}

inline
fn __stash_place_empty_blocks(
  reg u64 stash,
  #msf reg u64 msf
) -> #msf reg u64
{
  // standard variables
  reg u64 curr_bucket num_blocks;
  // pointer variables
  reg u64 blocks bucket_occupancy bucket_assignments;
  // temporary variables
  reg u64 tmp_bo bid i tmp_r offset;
  // boolean variables
  reg u8 c1 c2;
  reg bool b cond;
  inline int j;

  blocks, msf = stash_blocks(stash, msf);
  bucket_occupancy, msf = stash_bucket_occupancy(stash, msf);
  bucket_assignments, msf = stash_bucket_assignments(stash, msf);
  num_blocks, msf = stash_num_blocks(stash, msf);
  curr_bucket = #set0();
  i = 0;
  while { cond = i < num_blocks; } (cond) {
    msf = #update_msf(cond, msf);
    c1 = 0; // found_curr_bucket
    for j = 0 to PATH_LENGTH_0
    {
      c1 = !c1;
      // bucket_has_room
      #declassify tmp_bo = (64u)[bucket_occupancy + 8 * j];
      tmp_bo = #protect(tmp_bo, msf);
      b = tmp_bo != BLOCKS_PER_BUCKET;
      c2 = #SETcc(b);
      c2 &= c1;
      // set_curr_bucket
      b = c2 != 0;
      tmp_r = (64u)j;
      curr_bucket = #CMOVcc(b, tmp_r, curr_bucket);
      c1 = !c1;
      c1 |= c2;
    }
    tmp_bo = (64u)[bucket_occupancy + 8 * curr_bucket];
    offset = 8 * DECRYPTED_BLOCK_SIZE_QWORDS * i;
    bid = (64u)[blocks + offset];
    // cond_place_in_bucket
    b = bid == EMPTY_BLOCK_ID;
    c2 = #SETcc(b);
    b = tmp_bo < BLOCKS_PER_BUCKET;
    c1 = #SETcc(b);
    c1 &= c2;
    b = c1 != 0;

    tmp_r = #LEA(tmp_bo + 1);
    tmp_r = #CMOVcc(b, tmp_r, tmp_bo);
    (u64)[bucket_occupancy + 8 * curr_bucket] = tmp_r;
    tmp_bo = (64u)[bucket_assignments + 8 * i];
    tmp_r = #CMOVcc(b, curr_bucket, tmp_bo);
    (u64)[bucket_assignments + 8 * i] = tmp_r;
    i += 1;
  }
  msf = #update_msf(!cond, msf);
  // at the end, every bucket should be full
  return msf;
}

inline
fn __stash_assign_buckets(
  reg u64 stash path,
  #msf reg u64 msf
) -> #msf reg u64
{
  // standard variables
  reg u64 ub num_blocks;
  // pointer variables
  reg u64 bucket_assignments bucket_occupancy;
  // temporary variables
  reg u64 it;
  reg bool cond;
  inline int i lvl;

  // assign all blocks to "overflow" - level UINT64_MAX and set all occupancy to 0
  bucket_assignments, msf = stash_bucket_assignments(stash, msf);
  num_blocks, msf = stash_num_blocks(stash, msf);

  it = 0;
  while (it < num_blocks) { (u64)[bucket_assignments + 8 * it] = (64u)-1; it += 1; }
  bucket_occupancy, msf = stash_bucket_occupancy(stash, msf);
  for i = 0 to PATH_LENGTH_0 { (u64)[bucket_occupancy + 8 * i] = (64u)0; }

  // assign blocks in path to buckets first
  for lvl = 0 to PATH_LENGTH_0
  { for i = 0 to BLOCKS_PER_BUCKET
    {
      msf = _stash_assign_block_to_bucket_path(stash, path, lvl * BLOCKS_PER_BUCKET + i, msf);
    }
  }

  // assign blocks in overflow to buckets
  ub, msf = _stash_overflow_ub(stash, msf);
  it = 0;
  while { cond = it < ub; } (cond) {
    msf = #update_msf(cond, msf);
    msf = __stash_assign_block_to_bucket_overflow(stash, path, it, msf);
    it = #LEA(it + 1);
  }
  msf = #update_msf(!cond, msf);

  // now assign empty blocks to fill the buckets
  msf = __stash_place_empty_blocks(stash, msf);

  return msf;
}

inline
fn __stash_build_path(
  reg u64 stash path,
  #msf reg u64 msf
) -> #msf reg u64
{
  reg u64 overflow_size;
  reg u64 blocks bucket_assignments;

  msf = __stash_assign_buckets(stash, path, msf);

  blocks, msf = stash_blocks(stash, msf);
  overflow_size, msf = _stash_overflow_ub(stash, msf);
  overflow_size = #LEA(overflow_size + PATH_LENGTH_0 * BLOCKS_PER_BUCKET);

  bucket_assignments, msf = stash_bucket_assignments(stash, msf);

  msf = _odd_even_msort(blocks, bucket_assignments, 0, overflow_size, msf);

  return msf;
}

// oram
inline
fn __i_oram_read_path_for_block(
  reg u64 oram path,
  reg u64 target_block_id,
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target,
  reg u64 new_position,
  #msf reg u64 msf
) -> reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS], #msf reg u64
{
  reg u64 stash bucket_store value;
  inline int i;

  () = #spill(new_position);
  stash, msf = oram_stash(oram, msf);
  bucket_store, msf = oram_bucket_store(oram, msf);
  for i = 0 to PATH_LENGTH_0
  {
    #declassify value = (64u)[path + 8 + 8 * i];
    value = #protect(value, msf);
    target, msf = _i_stash_add_path_bucket(stash, bucket_store, value, target_block_id, target, msf);
  }
  target, msf = _i_stash_scan_overflow_for_target(stash, target_block_id, target, msf);

  () = #unspill(new_position);
  target[u64 0] = target_block_id;
  target[u64 1] = new_position;
  return target, msf;
}

inline
fn __oram_get(
  reg u64 oram,
  reg u64 block_id,
  reg u64 out_data,
  #msf reg u64 msf
) -> #msf reg u64
{
  stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
  // standard variables
  reg u64 new_position x;
  // pointer variables
  reg u64 stash path position_map bucket_store path_blocks;
  // temporary variables
  reg u64 bucket_id;
  inline int i max_position offset;

  () = #spill(out_data);
  target_block = target_block_s;
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
  max_position = (64u)(1 << (PATH_LENGTH_0 - 1));

  new_position = _random_mod_by_pow_of_2(max_position);
  msf = #init_msf();

  position_map, msf = oram_position_map(oram, msf);
  () = #spill(oram, target_block, block_id, new_position);
  x, msf = __position_map_read_then_set_1(position_map, block_id, new_position, msf);
  x *= 2;

  () = #unspill(oram, target_block, block_id, new_position);
  path, msf = oram_path(oram, msf);
  __tree_path_update(path, x);

  new_position *= 2;
  () = #spill(oram);
  target_block, msf = __i_oram_read_path_for_block(oram, path, block_id, target_block, new_position, msf);
  () = #spill(path);
  () = #unspill(out_data);
  target_block[2:BLOCK_DATA_SIZE_QWORDS] = __i_read_accessor(target_block[2:BLOCK_DATA_SIZE_QWORDS], out_data);

  () = #unspill(oram);
  stash, msf = oram_stash(oram, msf);
  () = #spill(oram);
  target_block, msf = _i_stash_add_block(stash, target_block, msf);

  () = #unspill(path);
  msf = __stash_build_path(stash, path, msf);
  () = #unspill(oram);

  bucket_store, msf = oram_bucket_store(oram, msf);
  path_blocks, msf = stash_path_blocks(stash, msf);
  offset = BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
  for i = 0 to PATH_LENGTH_0
  {
    #declassify bucket_id = [path + 8 + 8 * i];
    bucket_id = #protect(bucket_id, msf);
    msf = bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks, msf);
    path_blocks += (64u)offset;
  }

  return msf;
}

inline
fn __oram_put(
  reg u64 oram,
  reg u64 block_id start,
  reg u64 data prev_data,
  #msf reg u64 msf
) -> #msf reg u64
{
  stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
  // standard variables
  reg u64 new_position x;
  // pointer variables
  reg u64 stash path position_map bucket_store path_blocks;
  // temporary variables
  reg u64 bucket_id;
  inline int i max_position;

  () = #spill(block_id, start, data, prev_data);
  target_block = target_block_s;
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
  max_position = (64u)(1 << (PATH_LENGTH_0 - 1));

  new_position = _random_mod_by_pow_of_2(max_position);
  msf = #init_msf();
  new_position = new_position;

  position_map, msf = oram_position_map(oram, msf);
  () = #spill(oram, target_block, block_id, new_position);
  x, msf = __position_map_read_then_set_1(position_map, block_id, new_position, msf);
  x *= 2;

  () = #unspill(oram, target_block, block_id, new_position);
  path, msf = oram_path(oram, msf);
  __tree_path_update(path, x);

  new_position *= 2;
  () = #spill(oram);
  target_block, msf = __i_oram_read_path_for_block(oram, path, block_id, target_block, new_position, msf);
  () = #spill(path);
  () = #unspill(start, data, prev_data);
  target_block[2:BLOCK_DATA_SIZE_QWORDS] =
    _i_write_accessor_partial(target_block[2:BLOCK_DATA_SIZE_QWORDS], start, data, prev_data);

  () = #unspill(oram);
  stash, msf = oram_stash(oram, msf);
  () = #spill(oram);
  target_block, msf = _i_stash_add_block(stash, target_block, msf);

  () = #unspill(path);
  msf = __stash_build_path(stash, path, msf);
  () = #unspill(oram);

  bucket_store, msf = oram_bucket_store(oram, msf);
  path_blocks, msf = stash_path_blocks(stash, msf);
  for i = 0 to PATH_LENGTH_0
  {
    #declassify bucket_id = [path + 8 + 8 * i];
    bucket_id = #protect(bucket_id, msf);
    msf = bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks, msf);
    path_blocks += BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
  }

  return msf;
}

inline
fn __i_oram_put(
  reg u64 oram,
  reg u64 block_id start,
  reg u64 data,
  #msf reg u64 msf
) -> reg u64, #msf reg u64
{
  stack u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block_s;
  reg ptr u64[DECRYPTED_BLOCK_SIZE_QWORDS] target_block;
  // standard variables
  reg u64 new_position x prev_data;
  // pointer variables
  reg u64 stash path position_map bucket_store path_blocks;
  // temporary variables
  reg u64 bucket_id;
  inline int i max_position;

  () = #spill(start, data);
  target_block = target_block_s;
  for i = 0 to DECRYPTED_BLOCK_SIZE_QWORDS { target_block[i] = -1; }
  max_position = (64u)(1 << (PATH_LENGTH_0 - 1));

  new_position = _random_mod_by_pow_of_2(max_position);
  msf = #init_msf();
  new_position = new_position;

  position_map, msf = oram_position_map(oram, msf);
  () = #spill(oram, target_block, block_id, new_position);
  x, msf = __position_map_read_then_set_1(position_map, block_id, new_position, msf);
  x *= 2;
  
  () = #unspill(oram, target_block, block_id, new_position);
  oram = #protect(oram, msf);
  target_block = #protect_ptr(target_block, msf);
  path, msf = oram_path(oram, msf);
  __tree_path_update(path, x);

  new_position *= 2;
  () = #spill(oram);
  target_block, msf = __i_oram_read_path_for_block(oram, path, block_id, target_block, new_position, msf);
  () = #spill(path);
  () = #unspill(start, data);
  target_block[2:BLOCK_DATA_SIZE_QWORDS], prev_data =
    _i_write_accessor_partial_out(target_block[2:BLOCK_DATA_SIZE_QWORDS], start, data);
  () = #spill(prev_data);

  () = #unspill(oram);
  oram = #protect(oram, msf);
  stash, msf = oram_stash(oram, msf);
  () = #spill(oram);
  target_block, msf = _i_stash_add_block(stash, target_block, msf);

  () = #unspill(path);
  path = #protect(path, msf);
  msf = __stash_build_path(stash, path, msf);
  () = #unspill(oram);
  oram = #protect(oram, msf);

  bucket_store, msf = oram_bucket_store(oram, msf);
  path_blocks, msf = stash_path_blocks(stash, msf);
  for i = 0 to PATH_LENGTH_0
  {
    #declassify bucket_id = [path + 8 + 8 * i];
    bucket_id = #protect(bucket_id, msf);
    msf = bucket_store_write_bucket_blocks(bucket_store, bucket_id, path_blocks, msf);
    path_blocks += BLOCKS_PER_BUCKET * DECRYPTED_BLOCK_SIZE_QWORDS * 8;
  }

  () = #unspill(prev_data);
  return prev_data, msf;
}

// position map
inline
fn oram_position_map_get(
  reg u64 oram_position_map,
  reg u64 block_id,
  reg u64 position,
  #msf reg u64 msf
) -> #msf reg u64
{
  reg u64 buf bid_id_for_index idx_in_block position_map;

  bid_id_for_index, idx_in_block = block_id_for_index(oram_position_map, block_id);
  buf = (64u)[oram_position_map + 3 * 8];
  position_map = (64u)[oram_position_map + 8];

  () = #spill(position, idx_in_block, buf);
  msf = __oram_get(position_map, bid_id_for_index, buf, msf);
  
  () = #unspill(position, idx_in_block, buf);
  (u64)[position] = (64u)[buf + 8 * idx_in_block];

  return msf;
}

inline
fn oram_position_map_set(
  reg u64 oram_position_map,
  reg u64 block_id position,
  reg u64 prev_position,
  #msf reg u64 msf
) -> #msf reg u64
{
  reg u64 buf bid_id_for_index idx_in_block position_map;

  bid_id_for_index, idx_in_block = block_id_for_index(oram_position_map, block_id);
  buf = (64u)[oram_position_map + 3 * 8];
  position_map = (64u)[oram_position_map + 8];

  () = #spill(prev_position, buf, idx_in_block);
  msf = __oram_put(position_map, bid_id_for_index, idx_in_block, position, buf, msf);
  
  () = #unspill(prev_position, buf, idx_in_block);
  (u64)[prev_position] = [buf + 8 * idx_in_block];

  return msf;
}

inline
fn _i_oram_position_map_set(
  #public reg u64 oram_position_map,
  #secret reg u64 block_id,
  #secret reg u64 position,
  #msf reg u64 msf
) -> #secret reg u64, #msf reg u64
{
  reg u64 m_prime sh1 sh2 idx_in_block bid_for_index position_map prev_position;

  m_prime = [oram_position_map + 8 * EPB_CT_M_P_ADDR];
  sh1 = [oram_position_map + 8 * EPB_CT_SH1_ADDR];
  sh2 = [oram_position_map + 8 * EPB_CT_SH2_ADDR];

  idx_in_block = _ct_mod(block_id, BLOCK_DATA_SIZE_QWORDS, m_prime, sh1, sh2);

  bid_for_index = block_id_for_index_ct(oram_position_map, block_id);
  #declassify position_map = (64u)[oram_position_map + 8];

  prev_position, msf = __i_oram_put(position_map, bid_for_index, idx_in_block, position, msf);

  return prev_position, msf;
}
